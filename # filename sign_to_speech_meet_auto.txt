# filename: sign_to_speech_meet_auto.py
import cv2
import mediapipe as mp
import pyttsx3
import threading
import time
import numpy as np
import pyautogui
import webbrowser
import keyboard
from collections import deque

# ===================== CONFIG =====================
FRAME_WIDTH = 1280
FRAME_HEIGHT = 720
FRAME_FPS = 30
EMIT_COOLDOWN_SEC = 1.5
CONFIDENCE_THRESHOLD = 5  # consecutive frames to confirm gesture (higher = more accurate)

# Map gestures to phrases
GESTURE_TO_TEXT = {
    "FIVE": "Hello everyone!",
    "FIST": "Yes",
    "ONE": "No",
    "OK": "Thank you!",
    "ILY": "I love you all!",
    "PEACE": "Good morning!",
    "THUMBS_UP": "Great! üëç",
    "THUMBS_DOWN": "I disagree üëé",
    "CALL_ME": "Please call me",
    "POINT": "Look at this",
    "STOP": "Stop please",
    "HELP": "I need help",
    "ROCK": "Awesome! ü§ò",
    "THREE": "Okay",
}

# ===================== TTS =====================
class Speaker:
    def __init__(self, rate=150):   # ‚úÖ fixed constructor
        self.engine = pyttsx3.init()
        self.engine.setProperty("rate", rate)
        voices = self.engine.getProperty('voices')
        if voices:
            self.engine.setProperty('voice', voices[0].id)
        self.lock = threading.Lock()

    def say_async(self, text):
        def run():
            with self.lock:
                self.engine.say(text)
                self.engine.runAndWait()
        threading.Thread(target=run, daemon=True).start()

# ===================== GESTURE RECOGNITION =====================
mp_hands = mp.solutions.hands
mp_draw = mp.solutions.drawing_utils

def get_finger_state(landmarks, tip, pip, tol=0.02):
    """Check if finger is up with tolerance."""
    return landmarks[tip].y < (landmarks[pip].y - tol)

def get_thumb_state(landmarks, hand_side, tol=0.02):
    """Check if thumb is extended with tolerance."""
    if hand_side == "Right":
        return landmarks[4].x < (landmarks[3].x - tol)
    else:
        return landmarks[4].x > (landmarks[3].x + tol)

def recognize_gesture(landmarks, handedness):
    if not landmarks or not handedness:
        return None
    hand_side = "Right" if handedness.classification[0].label == "Right" else "Left"

    thumb_up = get_thumb_state(landmarks, hand_side)
    index_up = get_finger_state(landmarks, 8, 6)
    middle_up = get_finger_state(landmarks, 12, 10)
    ring_up = get_finger_state(landmarks, 16, 14)
    pinky_up = get_finger_state(landmarks, 20, 18)

    if all([thumb_up, index_up, middle_up, ring_up, pinky_up]):
        return "FIVE"
    if not any([thumb_up, index_up, middle_up, ring_up, pinky_up]):
        return "FIST"
    if index_up and not any([middle_up, ring_up, pinky_up, thumb_up]):
        return "ONE"
    if thumb_up and index_up and not any([middle_up, ring_up, pinky_up]):
        return "OK"
    if thumb_up and index_up and pinky_up and not middle_up and not ring_up:
        return "ILY"
    if index_up and middle_up and not any([thumb_up, ring_up, pinky_up]):
        return "PEACE"
    if thumb_up and not any([index_up, middle_up, ring_up, pinky_up]):
        return "THUMBS_UP"
    if not thumb_up and all([index_up, middle_up, ring_up, pinky_up]):
        return "THUMBS_DOWN"
    if thumb_up and pinky_up and not any([index_up, middle_up, ring_up]):
        return "CALL_ME"
    if index_up and not any([thumb_up, middle_up, ring_up, pinky_up]):
        return "POINT"
    if all([index_up, middle_up, ring_up, pinky_up]) and not thumb_up:
        return "STOP"
    if middle_up and ring_up and not any([thumb_up, index_up, pinky_up]):
        return "HELP"
    if index_up and pinky_up and not any([thumb_up, middle_up, ring_up]):
        return "ROCK"
    if thumb_up and index_up and middle_up and not any([ring_up, pinky_up]):
        return "THREE"
    return None

# ===================== MAIN =====================
def main():
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("‚ùå Cannot open webcam")
        return

    cap.set(cv2.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)
    cap.set(cv2.CAP_PROP_FPS, FRAME_FPS)

    speaker = Speaker(rate=150)
    last_phrase = ""
    last_emit_ts = 0
    current_gesture = None
    detection_count = 0
    meet_mode = False
    meet_opened = False

    # landmark smoothing buffer
    landmark_history = deque(maxlen=5)

    with mp_hands.Hands(
        max_num_hands=1,
        min_detection_confidence=0.8,
        min_tracking_confidence=0.7
    ) as hands:

        print("üì∑ Sign Language ‚Üí Google Meet Translator")
        print("ü§ö Show your hand to the camera")
        print("üìã Available gestures:", list(GESTURE_TO_TEXT.keys()))
        print("üîÑ Press 'M' to open Meet and enable Meet mode")
        print("ESC to quit")

        while True:
            ret, frame = cap.read()
            if not ret:
                break

            frame = cv2.flip(frame, 1)
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            res = hands.process(rgb)

            # ================== Open Meet ==================
            if keyboard.is_pressed('m'):
                if not meet_opened:
                    print("üåê Opening Google Meet...")
                    webbrowser.open("https://meet.google.com")
                    time.sleep(10)
                    meet_opened = True
                meet_mode = True
                print("üîÑ Meet mode ENABLED")
                time.sleep(1)

            gesture_name = ""
            phrase_now = ""
            confidence = 0

            if res.multi_hand_landmarks:
                handLms = res.multi_hand_landmarks[0]
                handedness = res.multi_handedness[0] if res.multi_handedness else None
                mp_draw.draw_landmarks(frame, handLms, mp_hands.HAND_CONNECTIONS)

                # smooth landmarks
                landmark_history.append(handLms.landmark)
                smoothed_landmarks = []
                for i in range(len(handLms.landmark)):
                    avg_x = np.mean([lm[i].x for lm in landmark_history])
                    avg_y = np.mean([lm[i].y for lm in landmark_history])
                    avg_z = np.mean([lm[i].z for lm in landmark_history])
                    smoothed_landmarks.append(type(handLms.landmark[0])(x=avg_x, y=avg_y, z=avg_z))

                g = recognize_gesture(smoothed_landmarks, handedness)

                if g == current_gesture:
                    detection_count += 1
                else:
                    current_gesture = g
                    detection_count = 1

                if detection_count >= CONFIDENCE_THRESHOLD and g in GESTURE_TO_TEXT:
                    phrase_now = GESTURE_TO_TEXT[g]
                    gesture_name = g
                    confidence = min(100, (detection_count / CONFIDENCE_THRESHOLD) * 100)

            t = time.time()
            if phrase_now and (phrase_now != last_phrase or (t - last_emit_ts) > EMIT_COOLDOWN_SEC):
                print(f"üó£ Detected: {phrase_now} ({gesture_name}, Confidence: {confidence:.1f}%)")
                speaker.say_async(phrase_now)
                if meet_mode:
                    # Send to Google Meet chat
                    pyautogui.hotkey('ctrl', 'alt', 'c')  # open chat
                    time.sleep(0.5)
                    pyautogui.write(phrase_now, interval=0.05)
                    pyautogui.press('enter')
                    time.sleep(0.5)
                    pyautogui.press('esc')  # close chat
                last_phrase = phrase_now
                last_emit_ts = t

            # ================== Display ==================
            mode_color = (0, 255, 0) if meet_mode else (0, 0, 255)
            mode_text = "MEET MODE: ON" if meet_mode else "MEET MODE: OFF"

            cv2.putText(frame, "Sign Language ‚Üí Google Meet", (30, 40),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
            cv2.putText(frame, mode_text, (30, 80),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, mode_color, 2)

            if gesture_name:
                cv2.putText(frame, f"Gesture: {gesture_name}", (30, 120),
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
                cv2.putText(frame, f"Confidence: {confidence:.1f}%", (30, 160),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 200, 200), 2)

            if phrase_now:
                cv2.putText(frame, f"Speaking & Sending: {phrase_now}", (30, 200),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

            cv2.imshow("Sign Language ‚Üí Google Meet", frame)

            if cv2.waitKey(1) & 0xFF == 27:  # ESC
                break

    cap.release()
    cv2.destroyAllWindows()

# ‚úÖ Corrected main check
if __name__ == "__main__":
    main()
